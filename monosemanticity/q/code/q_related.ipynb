{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_official_model_name\n",
      "Loaded pretrained model EleutherAI/pythia-160m into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1356103d7a47e4813711038eb7bb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/jx_projects/Lorsa/src')\n",
    "from typing import List, Set, Tuple, Dict, Union\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    ChameleonForConditionalGeneration,\n",
    "    PreTrainedModel,\n",
    ")\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.HookedTransformerConfig import HookedTransformerConfig\n",
    "from transformer_lens.components import Attention\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import einops\n",
    "\n",
    "from models.lorsa import LowRankSparseAttention\n",
    "from config import LorsaTrainConfig, LorsaConfig\n",
    "from analysis.new_analysis import sample_max_activating_sequences\n",
    "\n",
    "from lm_saes import (\n",
    "    ActivationFactoryActivationsSource,\n",
    "    ActivationFactoryConfig,\n",
    "    ActivationFactoryTarget,\n",
    "    ActivationFactory,\n",
    "    AnalyzeSAESettings,\n",
    "    SAEConfig,\n",
    "    SparseAutoEncoder,\n",
    "    FeatureAnalyzerConfig,\n",
    "    MongoDBConfig,\n",
    "    analyze_sae,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"EleutherAI/pythia-160m\"\n",
    "seq_len=256\n",
    "device='cuda'\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    device = 'cuda'\n",
    "    model_path = {\n",
    "        \"meta-llama/Llama-3.1-8B\": \"/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/models/Llama-3.1-8B\",\n",
    "        \"EleutherAI/pythia-160m\": \"/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/models/pythia-160m\",\n",
    "    }[model_name]\n",
    "    \n",
    "    dtype = {\n",
    "        \"meta-llama/Llama-3.1-8B\": torch.bfloat16,\n",
    "        \"EleutherAI/pythia-160m\": torch.float16,\n",
    "    }[model_name]\n",
    "\n",
    "    hf_model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        local_files_only=True,\n",
    "        torch_dtype=dtype,\n",
    "    ).to(device)\n",
    "\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True,\n",
    "        add_bos_token=True,\n",
    "        local_files_only=True,\n",
    "    )\n",
    "    hf_processor = None\n",
    "\n",
    "    model = HookedTransformer.from_pretrained_no_processing(\n",
    "        model_name,\n",
    "        use_flash_attn=False,\n",
    "        device=device,\n",
    "        hf_model=hf_model,\n",
    "        tokenizer=hf_tokenizer,\n",
    "        processor=hf_processor,\n",
    "        dtype=dtype,\n",
    "        hf_config=hf_model.config,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, hf_tokenizer\n",
    "\n",
    "model, tokenizer = load_model(model_name)\n",
    "model.eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dtype = {\n",
    "    \"meta-llama/Llama-3.1-8B\": torch.bfloat16,\n",
    "    \"EleutherAI/pythia-160m\": torch.float16,\n",
    "}[model_name]\n",
    "\n",
    "\n",
    "layer = 5\n",
    "lorsa_path = f'/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/jx_projects/Lorsa/result/pythia-160m/oneway_all_layer_result/L{layer}A'\n",
    "lorsa = LowRankSparseAttention.from_pretrained(\n",
    "    lorsa_path,\n",
    "    device='cuda',\n",
    ")\n",
    "lorsa.fold_W_O_into_W_V()\n",
    "lorsa.cfg.dtype=torch.float16\n",
    "lorsa.to(torch.float16)\n",
    "\n",
    "sae_dir = \"/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/jx_projects/Language-Model-SAEs/result/Pythia-160m-SAE-for-Lorsa\"\n",
    "sae_path = os.path.join(sae_dir, f\"L{layer}AIN\")\n",
    "sae_cfg=SAEConfig.from_pretrained(\n",
    "    pretrained_name_or_path=sae_path,\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "sae = SparseAutoEncoder.from_config(sae_cfg)\n",
    "\n",
    "# load dataset\n",
    "dataset_path = '/inspire/hdd/ws-8207e9e2-e733-4eec-a475-cfa1c36480ba/embodied-multimodality/public/zfhe/data/SlimPajama-3B'\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset['text'], \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "ignore_tokens = {\n",
    "    model.tokenizer.bos_token_id,\n",
    "    model.tokenizer.eos_token_id,\n",
    "    model.tokenizer.pad_token_id,\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_activation_with_filter_mask(\n",
    "    model: HookedTransformer,\n",
    "    batch: List[str],\n",
    "    ignore_tokens: Set[int],\n",
    "    cfg: LorsaConfig,\n",
    "    seq_len: int = 64,\n",
    ") -> Tuple[\n",
    "    Float[torch.Tensor, \"batch_size ctx_length d_model\"],\n",
    "    Bool[torch.Tensor, \"batch_size ctx_length\"],\n",
    "]:\n",
    "    tokens = model.to_tokens(\n",
    "        batch, \n",
    "        prepend_bos=True,\n",
    "    ).to(cfg.device, non_blocking=True)\n",
    "\n",
    "    tokens = tokens[:, :seq_len]\n",
    "\n",
    "    if len(ignore_tokens) > 0:\n",
    "        filter_mask = torch.any(\n",
    "            torch.stack(\n",
    "                [tokens == ignore_token for ignore_token in ignore_tokens], dim=0\n",
    "            ),\n",
    "            dim=0,\n",
    "        )  # This gives True on ignore tokens and False on informative ones.\n",
    "\n",
    "    hook_in_name = f'blocks.{cfg.layer}.ln1.hook_normalized'\n",
    "\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[hook_in_name])\n",
    "    hook_in = cache[hook_in_name]\n",
    "\n",
    "    return hook_in, ~filter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [03:17<00:00, 20.78sentences/s]\n"
     ]
    }
   ],
   "source": [
    "ov_head_index = 4065\n",
    "qk_head_index = int(ov_head_index * lorsa.cfg.n_qk_heads / lorsa.cfg.n_ov_heads)\n",
    "max_feature_mse = torch.zeros([sae.cfg.d_sae + 2], dtype=torch.float16).to(device)\n",
    "total_sampled_sentences = 256 * 16\n",
    "sampled_sentences = 0\n",
    "\n",
    "with tqdm(total=total_sampled_sentences, initial=0, unit='sentences') as pbar:\n",
    "    while sampled_sentences < total_sampled_sentences:\n",
    "        batch = next(data_iter)\n",
    "        hook_in, filter_mask = get_activation_with_filter_mask(model=model, batch=batch, ignore_tokens=ignore_tokens, cfg=lorsa.cfg, seq_len=seq_len)\n",
    "        hook_in = hook_in.to(dtype)\n",
    "        _, _, l1 = lorsa.cal_out_top_k_for_ov1(hook_in)\n",
    "        head_act_mask = filter_mask * (l1[..., ov_head_index]!=0)\n",
    "        head_act_index = torch.nonzero(head_act_mask)\n",
    "\n",
    "        for i in range(head_act_index.shape[0]):\n",
    "            batch_index = head_act_index[i][0]\n",
    "            q_index = head_act_index[i][1]\n",
    "            q_resid = hook_in[batch_index, q_index]\n",
    "            k_resid = hook_in[batch_index, :q_index+1]\n",
    "            q_resid_feature_acts = sae.encode(q_resid)\n",
    "            q_resid_feature_tensors = (q_resid_feature_acts.unsqueeze(dim=1) * sae.decoder.weight.T)\n",
    "            q_resid_feature_tensors = torch.cat([q_resid_feature_tensors, sae.decoder.bias.unsqueeze(0)], dim=0)\n",
    "            q_resid_feature_tensors = torch.cat([q_resid_feature_tensors, torch.zeros_like(q_resid_feature_tensors[:1])], dim=0)\n",
    "            q_resid_feature_tensors = q_resid - q_resid_feature_tensors\n",
    "            q = q_resid_feature_tensors @ lorsa.W_Q[qk_head_index] + lorsa.b_Q[qk_head_index]\n",
    "            k = k_resid @ lorsa.W_K[qk_head_index] + lorsa.b_K[qk_head_index]\n",
    "            q = lorsa.apply_rotary(q.reshape(q.shape[0], 1, 1, q.shape[1]).repeat([1, q_index+1, 1, 1])).squeeze()[:, q_index]\n",
    "            k = lorsa.apply_rotary(k.reshape(1, k.shape[0], 1, k.shape[1])).squeeze()\n",
    "            feature_attention_scores = q @ k.T / lorsa.cfg.attn_scale\n",
    "            feature_pattern = torch.softmax(feature_attention_scores, dim=1)\n",
    "            attention_scores = feature_attention_scores[-1]\n",
    "            pattern = feature_pattern[-1]\n",
    "            feature_mse = (feature_pattern - pattern).norm(p=2, dim=1)\n",
    "            max_feature_mse = torch.maximum(max_feature_mse, feature_mse)\n",
    "        pbar.update(batch_size)\n",
    "        sampled_sentences += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([1.0234, 0.7524, 0.7222, 0.6865, 0.6724, 0.6709, 0.6450, 0.6348, 0.6270,\n",
      "        0.6211], device='cuda:0', dtype=torch.float16),\n",
      "indices=tensor([6144, 5457, 3716, 1848, 5813, 1334, 2891, 1725, 2673, 5220],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(max_feature_mse.topk(k=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
